---
title: "Scalable single-effect Poisson regression"
author: "Yongjin Park"
---

Let us consider Poisson single-effect regression model for a certain column $k$ of a non-negative design matrix $X$:

$$Y_{i} \sim \operatorname{Poisson}(X_{ik} \theta_{k})$$

For $N$ data points, we define the likelihood:
 
$$L = \prod_{i=1}^{N} \frac{ (X_{ik} \theta_{k})^{Y_{i}} }{X_{ik}!} \exp\{- X_{ik} \theta_{k}\}.$$

For simplicity, we can assume Gamma prior conjugate to the Poisson likelihood:
$$\theta_{k} \sim \operatorname{Gamma}(a_{0}, b_{0}),$$

namely,
$$p(\theta_{k}|a_{0},b_{0}) = \frac{b_{0}^{a_{0}}}{\Gamma(a_{0})} \theta_{k}^{a_{0} - 1} \exp\{- b_{0} \theta_{k}\}$$


We can easily characterize posterior probability of each parameter:

$$\theta_{k} | Y, X \sim \operatorname{Gamma}\left(a_{0} + \sum_{i=1}^{N} Y_{i}, b_{0} + \sum_{i=1}^{N} X_{ik}\right)$$


A single effect regression model assumes that there is a single column $\mathbf{x}_{k}$ with non-zero coefficient $\theta_{k}$. We need to estimate the probability of choosing one $k$, but not the others. 


$$\log L(Y,X; \theta) = 
\sum_{i=1}^{N} Y_{i} \ln \left( \sum_{k=1}^{K} X_{ik} \theta_{k} \right) - \sum_{i=1}^{N} \sum_{k=1}^{K} X_{ik} \theta_{k}
+ \textsf{const.}$$


$$\log L \ge \sum_{k=1}^{K} z_{k} \sum_{i=1}^{N} Y_{i} \ln \frac{X_{ik} \theta_{k}}{z_{k}} -  \sum_{i=1}^{N} \sum_{k=1}^{K} X_{ik} \theta_{k}
+ \textsf{const.}$$

$$\ln z_{k} = \underbrace{\frac{ \sum_{i=1}^{N} Y_{i} \ln X_{ik} }{ \sum_{i=1}^{N} Y_{i} }}_{\textsf{empirical correlation}} + \underbrace{\ln \theta_{k}}_{\textsf{posterior log mean}}  + \textsf{const.}$$

Now we can analytically derive the update equation for the latent factor selection:

$$z_{k} = \operatorname{softmax}\left( \frac{\sum_{i=1}^{N} Y_{i} \ln X_{ik} }{ \sum_{i=1}^{N} Y_{i} } + \ln \theta_{k} \right)$$

Plugging this latent probability into the evidence lower bound, we can recalibrate the posterior of $\theta_{k}$:

$$\theta_{k}|Y,X \sim \operatorname{Gamma}\left(a_{0} + \sum_{i=1}^{N} Y_{i} z_{k}, b_{0} + \sum_{i=1}^{N} X_{ik} \right)$$


We can also consider a special case where $X_{ik} = 1$ for all $i$, denoting $k=0$, expecting that it would work as an intercept term. Since $\ln X_{ik} = 0$, we can easily derive that $z_{0} = \operatorname{softmax}(\ln \theta_{0})$ and $\theta_{0} \sim \operatorname{Gamma}(a_{0} + \sum_{i=1}^{N} Y_{i}, b_{0} + N)$

